{"cells":[{"cell_type":"markdown","id":"64ae4372","metadata":{"id":"64ae4372"},"source":["# Mutual Pointwise Information\n","\n","The code for this notebook has been prepared by **Aleksi Sahala** (University of Helsinki), 2019-2024, and is based on his research. It should be cited accordingly.\n","\n","[github.com/asahala](github.com/asahala)\n","\n","\n","## How to use?\n","\n","1. Create text object (text per line, lemmas separated by space)\n","```python\n","    text = Text('oracc-akkadian.txt')\n","```\n","\n","\n","2. Calculate co-occurrencies for the text object\n","```python\n","    cooc = Associations(text,\n","                 words1=['*'],            # All words to all words\n","                 formulaic_measure=Lazy,  # Use CSW\n","                 minfreq_b = 1,           # Min freq of b\n","                 minfreq_ab = 1,          # Min co-oc freq of a and b\n","                 symmetry=True,           # Window symmetry\n","                 windowsize=5,            # Window size\n","                 factorpower=2)           # k-value\n","```\n","\n","3. Calculate PMI from co-occurrences from the associations object\n","```python\n","    results = cooc.score(PMI2)            # Select association measure\n","```\n","\n","4. Print results from\n","```python\n","    x.print_scores(results, limit=1000, gephi=True, filename='oracc.pmi')\n","```"]},{"cell_type":"markdown","id":"8262b701","metadata":{"id":"8262b701"},"source":["## Imports and constants"]},{"cell_type":"code","execution_count":11,"id":"6d187a92","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:13.978751Z","start_time":"2024-05-21T20:23:13.971288Z"},"id":"6d187a92","executionInfo":{"status":"ok","timestamp":1716371146118,"user_tz":-180,"elapsed":1049,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["#from dictionary import dct\n","\n","import itertools\n","import json\n","import math\n","import re\n","import statistics\n","import sys\n","import time\n","from urllib.parse import quote\n","import random\n","from collections import Counter\n","\n","import requests\n","import tempfile\n","#import os\n"]},{"cell_type":"code","execution_count":12,"id":"a3af472f","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.029907Z","start_time":"2024-05-21T20:23:14.020319Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"a3af472f","executionInfo":{"status":"ok","timestamp":1716371155433,"user_tz":-180,"elapsed":1060,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}},"outputId":"a7ff477e-61cc-4905-b662-fe31e0d478a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["pmizer.py version 2024-05-19\n","\n"]}],"source":["__version__ = \"2024-05-19\"\n","\n","print('pmizer.py version %s\\n' % __version__)\n","\n","\"\"\" Constants \"\"\"\n","WINDOW_SCALING = True     # Apply window size penalty to scores\n","LOGBASE = 2               # Logarithm base; set to None for ln\n","LACUNAE = ['_']           # List of symbols for lacunae or removed words\n","LINEBREAK = '<LB>'        # Line break or text boundary\n","BUFFER = '<BF>'           # Buffer/padding symbol\n","INDENT = 4                # Indentation level for print\n","METASEPARATOR = '|'       # Character for separating multi-dimensional\n","                          # metadata for JSON\n","WRAPCHARS = ['[', ']']    # Wrap translations/POS-tags between these\n","                          # symbols, e.g. ['\"'] for \"string\". Give two\n","                          # if beginning and end symbols are different\n","DECIMALSEPARATOR = '.'    # Decimal separator for output files\n","HIDE_MIN_SCORE = True     # Hide minimum scores in matrices\n","VERBOSE = True            # Print more info\n","\n","IGNORE = [LINEBREAK, BUFFER]"]},{"cell_type":"markdown","id":"2d3d7fca","metadata":{"id":"2d3d7fca"},"source":["## Functions and classes\n","### Logarithm base definition"]},{"cell_type":"code","execution_count":13,"id":"6fddb958","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:44:36.353200Z","start_time":"2024-05-21T20:44:36.344658Z"},"id":"6fddb958","executionInfo":{"status":"ok","timestamp":1716371158891,"user_tz":-180,"elapsed":324,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["# log-base 2 should be used by default\n","\n","def _log(n):\n","    if LOGBASE is None:\n","        return math.log(n)\n","    else:\n","        return math.log(n, LOGBASE)\n","\n","\n","def make_korp_oracc_url(w1, w2, wz):\n","    \"\"\" Generate URL for Oracc in Korp \"\"\"\n","    w1 = re.sub('(.+)_.+?', r'\\1', w1)\n","    w2 = re.sub('(.+)_.+?', r'\\1', w2)\n","\n","    w1 = w1.split('[')[0]\n","    w2 = w2.split('[')[0]\n","\n","    base = 'https://www.kielipankki.fi/korp/?mode=other_languages#'\\\n","           '?lang=en&stats_reduce=word'\n","    cqp = '&cqp=%5Blemma%20%3D%20%22{w1}%22%5D%20%5B%5D%7B0,'\\\n","          '{wz}%7D%20%5Blemma%20%3D%20%22{w2}%22%5D'\\\n","          .format(w1=quote(w1), w2=quote(w2), wz=wz)\n","    corps = '&corpus=oracc_adsd,oracc_ario,oracc_blms,oracc_cams,oracc_caspo,oracc_ctij'\\\n","            ',oracc_dcclt,oracc_dccmt,oracc_ecut,oracc_etcsri,oracc_hbtin,oracc_obmc,'\\\n","            'oracc_riao,oracc_ribo,oracc_rimanum,oracc_rinap,oracc_saao,'\\\n","            'oracc_others&search_tab=1&search=cqp&within=paragraph'\n","\n","    ## TEMPORARY FOR ALP COURSE, REMOVE THE LINE BELOW\n","    corps = '&corpus=oracc2021_rinap&search_tab=1&search=cqp&within=paragraph'\n","\n","    return base+cqp+corps"]},{"cell_type":"markdown","id":"7e8132b1","metadata":{"id":"7e8132b1"},"source":["### Input / Output tools"]},{"cell_type":"code","execution_count":14,"id":"e71cf9b3","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.045167Z","start_time":"2024-05-21T20:23:14.039164Z"},"id":"e71cf9b3","executionInfo":{"status":"ok","timestamp":1716371162522,"user_tz":-180,"elapsed":1,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["class IO:\n","\n","    \"\"\" Basic file IO-operations and verbose \"\"\"\n","\n","    def read_file(filename):\n","        print(': Reading %s' % filename)\n","        try:\n","            with open(filename, 'r', encoding='utf-8', errors='ignore') as data:\n","                return data.read().splitlines()\n","        except FileNotFoundError:\n","            IO.errormsg(\"File not found: %s\" % filename)\n","            sys.exit(0)\n","\n","    def write_file(filename, content):\n","        with open(filename, 'w', encoding='utf-8') as data:\n","            data.write(content)\n","        print(': Saved %s' % filename)\n","\n","    def export_json(filename, content):\n","        \"\"\" Save lookup table as JSON \"\"\"\n","        with open(filename, 'w', encoding=\"utf-8\") as data:\n","            json.dump(content, data)\n","        print(': Saved %s' % filename)\n","\n","    def import_json(filename):\n","        \"\"\" Load lookup table from JSON \"\"\"\n","        try:\n","            print(': Reading %s' % filename)\n","            with open(filename, encoding='utf-8') as data:\n","                return json.load(data)\n","        except FileNotFoundError:\n","            IO.errormsg(\"File not found: %s\" % filename)\n","            sys.exit(0)\n","\n","    def show_time(time_, process):\n","        if VERBOSE:\n","            print(\"%s%s took %0.2f seconds\" \\\n","                  % (\" \"*INDENT, process, round(time_, 2)))\n","\n","    def printmsg(message):\n","        if VERBOSE:\n","            print(message)\n","\n","    @staticmethod\n","    def errormsg(message):\n","        print(\": Error! %s\" % message)"]},{"cell_type":"markdown","id":"a60d5881","metadata":{"id":"a60d5881"},"source":["### Word association measures\n","\n","Measures take four arguments:\n","\n","*   ab = co-oc freq\n","*   a  = freq of a\n","*   b  = freq of b\n","*   cz = corpus size\n","*   factor = CSW value (this is used if postweight is set)\n","*   oo = estimated number of all bigrams in the corpus"]},{"cell_type":"code","execution_count":15,"id":"2ae374c6","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.063890Z","start_time":"2024-05-21T20:23:14.046603Z"},"id":"2ae374c6","executionInfo":{"status":"ok","timestamp":1716371166666,"user_tz":-180,"elapsed":299,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["## POINTWISE MUTUAL INFORMATION BASED MEASURES\n","\n","class PMI:\n","    \"\"\" Pointwise Mutual Information. The score orientation is\n","    -log p(a,b) > 0 > -inf. As in Church & Hanks 1990. \"\"\"\n","    minimum = -math.inf\n","\n","    @staticmethod\n","    def raw(ab, a, b, cz, oo=None):\n","        return (ab/cz) / ((a/cz)*(b/cz))\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return factor * (_log(ab*cz) - _log(a*b))\n","\n","class PMIDELTA:\n","    \"\"\" Smooth PMI (Pantel & Lin 2002). This measure reduces\n","    the PMI score more, the rarer the words are, thus reducing\n","    the low-frequency bias \"\"\"\n","    minimum = -math.inf\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        weight = (ab/(ab+1)) * (min(a,b)/(min(a,b,)+1))\n","        return factor * weight * (_log(ab*cz) - _log(a*b))\n","\n","class PMICDS:\n","    \"\"\" Context distribution smoothed PMI (Levy, Goldberg &\n","    Dagan 2015). This measure raises the f(b) to the power of\n","    alpha = 0.75 \"\"\"\n","    minimum = -math.inf\n","\n","    ## WORKS ONLY IN MATRIX FACTORIZATION\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        alpha = 0.75\n","        return max(factor * _log((cz*ab) / (a*(b**alpha))),0)\n","\n","class NPMI:\n","    \"\"\" Normalized PMI. The score orientation is  +1 > 0 > -1\n","    as in Bouma 2009 \"\"\"\n","    minimum = -1.0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return factor * (PMI.score(ab, a, b, cz, 1) / -_log(ab/cz))\n","\n","\n","class PMISIG:\n","    \"\"\" Washtell & Markert (2009). \"\"\"\n","    minimum = -math.inf\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        pa = a / cz\n","        pb = b / cz\n","        return factor * (math.sqrt(min(pa, pb))\\\n","                         * (PMI.raw(ab, a, b, cz)))\n","\n","class SCISIG:\n","    \"\"\" Washtell & Markert (2009) The original publication does\n","    not tell the score orientation, but it can be shown to be\n","    +1 > √p(a,b) > 0 \"\"\"\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        pa = a / cz\n","        pb = b / cz\n","        pab = ab / cz\n","        return factor * (math.sqrt(min(pa, pb))\\\n","                         * (pab / ((pa) * math.sqrt(pb))))\n","\n","\n","class cPMI:\n","    \"\"\" Corpus Level Significant PMI as in Damani 2013. According to\n","    the original research paper, delta value of 0.9 is recommended \"\"\"\n","    minimum = -math.inf\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        delta = 0.9\n","        t = math.sqrt(_log(delta) / (-2 * a))\n","        return _log(ab / (a * b / cz) + a * t) * factor\n","\n","\n","class PMI2:\n","    \"\"\" PMI^2. Fixes the low-frequency bias of PMI and NPMI by squaring\n","    the numerator to compensate multiplication done in the denominnator.\n","    Scores are oriented as: 0 > log p(a,b) > -inf. As in Daille 1994 \"\"\"\n","    minimum = -math.inf\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return (PMI.score(ab, a, b, cz, 1) - (-_log(ab/cz))) / factor\n","\n","\n","class PMI3:\n","    \"\"\" PMI^3 (no low-freq bias, favors common bigrams). Although not\n","    mentioned in any papers at my disposal, the scores are oriented\n","    log p(a,b) > 2 log p(a,b) > -inf. As in Daille 1994\"\"\"\n","    minimum = -math.inf\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return (PMI.score(ab, a, b, cz, 1) - (-(2*_log(ab/cz)))) / factor\n","\n","class SPMI:\n","    \"\"\" Positive shifted PMI. Works as the regular PMI but discards negative\n","    scores: -log p(a,b) > 0 = 0; Shift by 3 \"\"\"\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return max(3 + PMI.score(ab, a, b, cz, factor), 0)\n","\n","class PPMI:\n","    \"\"\" Positive PMI. Works as the regular PMI but discards negative\n","    scores: -log p(a,b) > 0 = 0 \"\"\"\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return max(PMI.score(ab, a, b, cz, factor), 0)\n","\n","\n","class PPMI2:\n","    \"\"\" Positive derivative of PMI^2 as in Role & Nadif 2011.\n","    Shares exaclty the same properties but the score orientation\n","    is on the positive plane: 1 > 2^log p(a,b) > 0 \"\"\"\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return factor * (2 ** PMI2.score(ab, a, b, cz, 1))\n","\n","class PPMI3:\n","    \"\"\" Positive derivative of PMI^3. Shares exaclty the same properties\n","    but the score orientation is: p(a,b) > p(a,b)^2 > 0\n","\n","    Not mentioned in Role & Nadif \"\"\"\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return factor * (2 ** PMI3.score(ab, a, b, cz, 1))\n","\n","\n","class NPMI2:\n","    \"\"\" NPMI^2. Removes the low-frequency bias as PMI^2 and has\n","    a fixed score orientation as NPMI: 1 > 0 = 0. Take cube root\n","    of the result to trim excess decimals. Sahala & Linden (2020) \"\"\"\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        ind = 2 ** _log(ab / cz)\n","        base_score = 2 ** PMI2.score(ab, a, b, cz, 1) - ind\n","        return (max(base_score / (1 - ind), 0) * factor) ** (1/3)\n","\n","\n","class NPMI3:\n","    \"\"\" NPMI^3. Removes the low-frequency bias as PMI^3 and has\n","    a fixed score orientation as NPMI: 1 > 0 = 0. Take cube root\n","    of the result to trim excess decimals. Sahala & Linden (2020) \"\"\"\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        pab = (ab/cz)\n","        base_score = 2 ** PMI3.score(ab, a, b, cz, 1) - (pab**2)\n","        return (max(base_score / (pab - (pab**2)), 0) * factor) ** (1/3)"]},{"cell_type":"markdown","id":"d4f4fc3a","metadata":{"id":"d4f4fc3a"},"source":["### Other measures and statistical tests"]},{"cell_type":"code","execution_count":16,"id":"df7ab436","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.075778Z","start_time":"2024-05-21T20:23:14.065513Z"},"id":"df7ab436","executionInfo":{"status":"ok","timestamp":1716371173747,"user_tz":-180,"elapsed":501,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["class NormalizedExpectation:\n","    \"\"\" Normalized Expectation as in Pecina 2006\n","\n","       2f(xy) / f(x)f(y) \"\"\"\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        return (2*ab*factor) / (a+b)\n","\n","\n","class tTest:\n","    \"\"\" Student's t-test \"\"\"\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        ind = a*b/cz\n","        return ((ab*factor)-ind) / math.sqrt(factor*ab*(1-(factor*ab/cz)))\n","\n","\n","class zScore:\n","    \"\"\" Z-score \"\"\"\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        ind = a*b/cz\n","        return ((ab*factor)-ind) / math.sqrt(ind*(1-(ind/cz)))\n","\n","# Association coefficients, See Pecina 2006\n","\n","def c_table(ab, a, b, cz, factor, oo=None):\n","    \"\"\" Return contingency table. Note that the total number of\n","    co-occurrences in the corpus is based on an estimate, because\n","    not all bigrams are calculated for efficiency \"\"\"\n","    ab = ab*factor\n","    A = ab\n","    B = a - ab\n","    C = b - ab\n","    D = oo - ab\n","    return A, B, C, D\n","\n","\n","class Jaccard:\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n","        return A / (A+B+C)\n","\n","\n","class Odds:\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n","        return (A*D)/(B*C)\n","\n","\n","\n","class Simpson:\n","    \"\"\" Note: this has an extreme low-freq bias \"\"\"\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n","\n","        return A / min(A+B, A+C)\n","\n","\n","class BraunBlanquet:\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n","\n","        return A / max(A+B, A+C)\n","\n","\n","class Pearson:\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n","        u = (A * D) - (B * C)\n","        d = math.sqrt((A+B)*(A+C)*(D+B)*(D+C))\n","        return u / d\n","\n","\n","class UnigramSubtuples:\n","\n","    minimum = 0\n","\n","    @staticmethod\n","    def score(ab, a, b, cz, factor, oo=None):\n","        A, B, C, D = c_table(ab, a, b, cz, factor, oo)\n","\n","        subs = [1/A, 1/B, 1/C, 1/D]\n","        return _log((A*D)/(B*C)) - (3.29 * math.sqrt(sum(subs)))\n","\n"]},{"cell_type":"markdown","id":"8eee9cc1","metadata":{"id":"8eee9cc1"},"source":["### Context Similarity Weighting\n","\n","This is an initial version of CSW as in Sahala & Linden (2020); performs similarly\n","but has a space complexity of `O(n×m^2) (n = corpus size, m = window len)`\n","\n","Sahala, Aleksi & Krister Lindén. 2020. [Improving Word Association Measures in Repetitive Corpora with Context Similarity Weighting](https://researchportal.helsinki.fi/files/158886805/KDIR_2020_12_CR.pdf), in: A L N Fred & J Filipe (eds.), *Proceedings of the 12th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2020, Volume 1: KDIR, Budapest, Hungary, November 2-4, 2020.* SCITEPRESS Science And Technology Publications, Setúbal, 48–58, https://doi.org/10.5220/0010106800480058"]},{"cell_type":"code","execution_count":17,"id":"b98c4333","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.081155Z","start_time":"2024-05-21T20:23:14.077057Z"},"id":"b98c4333","executionInfo":{"status":"ok","timestamp":1716371179236,"user_tz":-180,"elapsed":280,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["class FormulaicMeasures:\n","    @staticmethod\n","    def compensate(words, collocate, uniqs):\n","        \"\"\" Compensate window matrix counts:\n","        ´compensation´ is a number of each word that should be\n","        ignored in counting (metasymbols, collocates) \"\"\"\n","        compensation = uniqs\n","        for symbol in LACUNAE + [BUFFER, LINEBREAK, collocate]:\n","            compensation += max(words.count(symbol) - 1, 0)\n","        return compensation\n","\n","\n","class Greedy:\n","    \"\"\" Removed temporarily \"\"\"\n","    pass\n","\n","\n","class Lazy:\n","    @staticmethod\n","    def score(windows, collocate):\n","        diffs = []\n","        for window in zip(*windows[::-1]):\n","            \"\"\" Count the number of unique words in transposed window matrix\n","            ie. stack windows and count uniques, see compensate() for more\n","            info \"\"\"\n","            uniqs = len(set(window))\n","            compensated = FormulaicMeasures.compensate(window, collocate, uniqs)\n","            diffs.append((len(window) - compensated) / len(window))\n","            \"\"\" Uncomment to see probabilities \"\"\"\n","            #print('\\t'.join(window), (len(window) - compensated) / len(window))\n","        return sum(diffs) / max((len(diffs) - 1),1)"]},{"cell_type":"markdown","id":"b27601a5","metadata":{"id":"b27601a5"},"source":["### Text container and basic text analysis tools"]},{"cell_type":"code","execution_count":18,"id":"1880a634","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.104012Z","start_time":"2024-05-21T20:23:14.082536Z"},"id":"1880a634","executionInfo":{"status":"ok","timestamp":1716371183242,"user_tz":-180,"elapsed":300,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["class Text(object):\n","\n","    def __init__(self, filename, ignore=LACUNAE):\n","        self.filename = filename\n","        self.content = []\n","        self.content_uniq = []\n","        self.metadata = []\n","        self.documents = []\n","        self.translations = {}\n","        self.ignore = ignore\n","\n","        self._read(filename)\n","\n","    def __repr__(self):\n","        return self.stats\n","\n","    @staticmethod\n","    def _tokenize(string):\n","        return string.strip().split(' ')\n","\n","    @staticmethod\n","    def _count(symbols, line):\n","        return len([s for s in line if s in symbols])\n","\n","    def _read(self, filename):\n","\n","        st = time.time()\n","\n","        \"\"\" Read input text from lemmatized raw text file \"\"\"\n","        metalength = None\n","        self.lacunacount = 0\n","        self.maxlen = 0\n","        self.linecount = 0\n","        self.corpus_size = 0\n","\n","        for line in IO.read_file(self.filename):\n","            if line:\n","                self.linecount += 1\n","                fields = line.split('\\t')\n","                text = fields[-1]\n","\n","                if metalength is None:\n","                    metalength = [len(fields)]\n","                elif len(fields) not in metalength:\n","                    IO.errormsg('(%s at line %i): Inconsistent '\\\n","                          'number of metadata fields.' % (self.filename,\n","                                                          self.linecount))\n","                    sys.exit(0)\n","\n","                if len(fields) > 1:\n","                    \"\"\" Collect metadata \"\"\"\n","                    meta = METASEPARATOR.join(fields[0:-1])\n","                    self.metadata.append(meta)\n","\n","                lemmas = self._tokenize(text)\n","                \"\"\" Store documents for TF-IDF \"\"\"\n","                self.documents.append(lemmas)\n","                self.lacunacount += self._count(self.ignore, lemmas)\n","\n","                \"\"\" Lacunae are count as lemmas but buffers are not \"\"\"\n","                lemmacount = len(lemmas)\n","                self.corpus_size += lemmacount\n","                if lemmacount > self.maxlen:\n","                    self.maxlen = lemmacount\n","\n","                self.content.extend([LINEBREAK] + [BUFFER] + lemmas)\n","\n","        \"\"\" Add buffer to the end of the file \"\"\"\n","        self.content.extend([BUFFER])\n","\n","        \"\"\" Make frequency list \"\"\"\n","        self.word_freqs = Counter(self.content)\n","\n","        st2 = time.time() - st\n","        IO.show_time(st2, \"Reading\")\n","\n","        IO.printmsg(self.stats)\n","\n","    @property\n","    def stats(self):\n","        \"\"\" Return corpus statistics \"\"\"\n","        tab = INDENT * ' '\n","        non_words = [BUFFER, LINEBREAK]\n","        freqs = sorted([f for f in self.word_freqs.values()])\n","        log = [('\\n: Text statistics:'),\n","               ('%sLines: %i' % (tab, self.linecount)),\n","               ('%sLongest line: %i' % (tab, self.maxlen)),\n","               ('%sWord count: %i' % (tab, self.corpus_size)),\n","               ('%sWord count (non-lacunae): %i' \\\n","                % (tab, self.corpus_size - self.lacunacount)),\n","               ('%sLacunae or ignored symbols: %i' % (tab, self.lacunacount)),\n","               ('%sUnique words: %i' \\\n","                % (tab, len(self.word_freqs.keys()) - len(non_words))),\n","               ('%sMedian word frequency: %i' \\\n","                % (tab, statistics.median(freqs))),\n","               ('%sAverage word frequency: %.2f' \\\n","                % (tab, sum(freqs) / len(freqs)))]\n","        return '\\n'.join(log) + '\\n'\n","\n","    @property\n","    def metadata_stats(self):\n","        \"\"\" Return word and line counts for each metadata group \"\"\"\n","        if not self.metadata:\n","            return None\n","\n","        meta = {}\n","        tab = ' '*INDENT\n","        for index, content in enumerate(self.documents):\n","            wordcount = len(content)\n","            meta.setdefault(self.metadata[index], {'words': 0, 'lines': 0})\n","            meta[self.metadata[index]]['words'] += wordcount\n","            meta[self.metadata[index]]['lines'] += 1\n","\n","        return '\\n'+'\\n'.join([('%s%s\\t%i\\t%i' \\\n","                % (tab, k.replace(METASEPARATOR, '\\t'), v['words'], v['lines']))\\\n","                          for k, v in sorted(meta.items())]) + '\\n'\n","\n","    def iterate(self, windowsize=1):\n","        \"\"\" Iterate content of the text and extend buffers to match\n","        window size \"\"\"\n","        for word in self.content:\n","            if word == BUFFER:\n","                for i in range(0, windowsize):\n","                    yield word\n","            else:\n","                yield word\n","\n","\n","    def tf_idf(self, threshold=0):\n","        \"\"\" Returns a TF-IDF based stopword list based on the text.\n","        This can be passed to Associations() as a keyword argument\n","\n","        ´threshold´ defines the size of the list, if no argument is\n","        give, will return a list relative to corpus size \"\"\"\n","\n","        print(': Making TF-IDF stopword list')\n","\n","        st = time.time()\n","        tf_idfs = {}\n","        words = []\n","\n","        if threshold == 0:\n","            threshold = int(0.000005 * self.corpus_size)\n","\n","        for document in self.documents:\n","            N = len(document)\n","\n","            for word in set(document):\n","                t = document.count(word)\n","                tf_idfs.setdefault(word, {'tf': [], 'found_in': 0})\n","                tf_idfs[word]['tf'].append(t/N)\n","                tf_idfs[word]['found_in'] += 1\n","\n","        for word, vals in tf_idfs.items():\n","            scores = []\n","            for tf in vals['tf']:\n","                scores.append(tf * math.log(len(self.documents)/vals['found_in'], 10))\n","            words.append([sum(scores), word])\n","\n","        st2 = time.time() - st\n","        IO.show_time(st2, \"TF-IDF\")\n","        return [x[1] for x in sorted(words, reverse=True)[0:threshold]]\n","\n","    def read_dict(self):\n","        filename = self.filename.split('.')[0] + '.dict'\n","        with open(filename, 'r', encoding='utf-8', errors='ignore') as data:\n","            for line in data.read().splitlines():\n","                key, value = line.split('\\t')\n","                self.translations[key] = value\n","\n","    def uniquify(self, wz):\n","        # This feature is not finished\n","\n","        \"\"\" Produce a version of text that do not need window scaling.\n","        Iterate text and disallow words occurring more than once\n","        withing a given distance from each other. Replace non-unique\n","        words with lacunae \"\"\"\n","        print(': Uniquifying windows')\n","        st = time.time()\n","        count = 0\n","        count_non_lacunae = 0\n","        removed = 0\n","        for word in self.content:\n","            \"\"\" Initialize buffer \"\"\"\n","            if word == LINEBREAK:\n","                buffer = []\n","            \"\"\" Keep buffer length \"\"\"\n","            if len(buffer) == wz + 1:\n","                buffer.pop(0)\n","            \"\"\" Replace non-uniques with lacunae \"\"\"\n","            if word in buffer and word not in LACUNAE + IGNORE:\n","                removed += 1\n","                word = LACUNAE[0]\n","\n","            if word not in IGNORE:\n","                count += 1\n","                if word not in LACUNAE:\n","                    count_non_lacunae += 1\n","            buffer.append(word)\n","            self.content_uniq.append(word)\n","\n","        self.corpus_size_uniq = Counter(self.content_uniq)\n","\n","        st2 = time.time() - st\n","        IO.show_time(st2, \"Uniquifying\")\n","        print(\"%s--> %i words removed\" \\\n","              % (' '*INDENT, removed))\n","\n","        #for k, v in sorted(Counter(removed).items()):\n","        #    print(v, k)\n","\n","    \"\"\" ================================================================\n","    Random sampling tools (for measure evaluation purposes)\n","    ================================================================ \"\"\"\n","\n","    \"\"\" Sample a population of words from frequency list from\n","    given frequency range. ´quantity´ is the population size and\n","    ´freq_range´ a list that contains the min and max freq,\n","    e.g. [30,50] \"\"\"\n","\n","    def pick_random(self, quantity, freq_range):\n","        sampled = []\n","        for k, v in self.word_freqs.items():\n","            if freq_range[1] > v > freq_range[0]:\n","                sampled.append(k)\n","        self.random_sample = random.sample(sampled, quantity)\n","        return self.random_sample\n","\n","    \"\"\" Random samples can be saved and loaded with the\n","    following funtions \"\"\"\n","\n","    def save_random(self, filename):\n","        with open(filename, 'w', encoding='utf-8') as data:\n","            data.write('\\n'.join(self.random_sample))\n","\n","    def load_random(self, filename):\n","        with open(filename, 'r', encoding='utf-8') as data:\n","            self.random_sample = data.read().splitlines()\n","            return self.random_sample"]},{"cell_type":"markdown","id":"430cb7c7","metadata":{"id":"430cb7c7"},"source":["### Association measure tools"]},{"cell_type":"code","execution_count":19,"id":"8e8c1531","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.163133Z","start_time":"2024-05-21T20:23:14.108097Z"},"id":"8e8c1531","executionInfo":{"status":"ok","timestamp":1716371191092,"user_tz":-180,"elapsed":327,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"outputs":[],"source":["class Associations:\n","\n","    def __init__(self, text, **kwargs):\n","        if not isinstance(text, Text):\n","            IO.errormsg('Association must have Text object as argument.')\n","            sys.exit(0)\n","\n","        self.text = text\n","        self.word_freqs = self.text.word_freqs\n","        self.corpus_size = self.text.corpus_size\n","\n","        self.windowsize = 2\n","        self.minfreq_b = 1\n","        self.minfreq_ab = 1\n","\n","        self.distances = {}\n","        self.WINS = {}\n","\n","        self.translations = {}\n","        if self.text.translations:\n","            self.translations = self.text.translations\n","\n","        self.track_distance = False\n","        self.symmetry = False\n","        self.track_distance = False\n","        self.positive_condition = False\n","        self.formulaic_measure = None\n","        self.postweight = False\n","        self.factorpower = 1\n","\n","        self.words = {1: [], 2: []}\n","        self.regex_words = {1: [], 2: []}\n","\n","        self.metadata = {}\n","\n","        self.conditions = {'stopwords': LACUNAE + ['', BUFFER, LINEBREAK],\n","                           'stopwords_regex': [],\n","                           'conditions': [],\n","                           'conditions_regex': []}\n","\n","        self.set_constraints(**kwargs)\n","        self.count_bigrams()\n","\n","        if self.corpus_size < self.windowsize:\n","            IO.errormsg('Window size exceeds corpus size.')\n","            sys.exit(1)\n","\n","    def __repr__(self):\n","        \"\"\" Define what is not shown in .log files \"\"\"\n","        debug = []\n","        tab = max([len(k)+2 for k in self.__dict__.keys()])\n","        for k in sorted(self.__dict__.keys()):\n","            if k not in ['scored', 'text', 'regex_stopwords', 'metadata',\n","                         'regex_words', 'distances', 'anywords',\n","                         'anywords1', 'output', 'anywords2', 'bigram_freqs',\n","                         'anycondition', 'word_freqs', 'positive_condition',\n","                         'minimum', 'WINS', 'documents', 'translations']:\n","                v = self.__dict__[k]\n","                debug.append('%s%s%s' % (k, ' '*(tab-len(k)+1), str(v)))\n","\n","        return '\\n'.join(debug) + '\\n' + '-'*20 +\\\n","               ' \\npmizer version: ' + __version__\n","\n","    def set_constraints(self, **kwargs):\n","        \"\"\" Set constraints. Separate regular expressions from the\n","        string variables, as string comparison is significantly faster\n","        than re.match() \"\"\"\n","\n","        for key, value in kwargs.items():\n","            if key in ['stopwords', 'conditions']:\n","                for word in value:\n","                    if isinstance(word, str):\n","                        self.conditions[key].append(word)\n","                    else:\n","                        self.conditions[key+'_regex'].append(word)\n","            elif key in ['words1', 'words2']:\n","                index = int(key[-1])\n","                for word in value:\n","                    if isinstance(word, str):\n","                        self.words[index].append(word)\n","                    else:\n","                        self.regex_words[index].append(word)\n","            else:\n","                setattr(self, key, value)\n","\n","        \"\"\" Combine tables for faster comparison \"\"\"\n","        self.anywords = any([self.words[1], self.words[2],\n","                         self.regex_words[1], self.regex_words[2]])\n","        self.anywords1 = any([self.words[1], self.regex_words[1]])\n","        self.anywords2 = any([self.words[2], self.regex_words[2]])\n","        self.anycondition = any([self.conditions['conditions'],\n","                                 self.conditions['conditions_regex']])\n","\n","    \"\"\" ================================================================\n","    Helper funtions ====================================================\n","    ================================================================ \"\"\"\n","\n","    def _trim_float(self, number):\n","        if not number:\n","            return number\n","        elif isinstance(number, int):\n","            return number\n","        else:\n","            return round(number, 3)\n","\n","    def _get_translation(self, word):\n","        \"\"\" Get translation from dictionary \"\"\"\n","        try:\n","            translation = '%s%s%s' % (WRAPCHARS[0], self.translations[word], WRAPCHARS[-1])\n","        except:\n","            translation = '%s?%s' % (WRAPCHARS[0], WRAPCHARS[-1])\n","        return translation\n","\n","    def _get_distance(self, bigram):\n","        \"\"\" Calculate average distance for bigram's words; if not\n","        used, the distance will be equal to window size. \"\"\"\n","        if self.track_distance:\n","            distance = self._trim_float(sum(self.distances[bigram])\n","                                    / len(self.distances[bigram]))\n","        else:\n","            distance = ''\n","        return distance\n","\n","    def _match_regex(self, words, regexes):\n","        \"\"\" Matches a list of regexes to list of words \"\"\"\n","        return any([re.match(r, w) for r in regexes for w in words])\n","\n","    def _meets_anycondition(self, condition, words):\n","        \"\"\" Compare words with stopword/conditions list and regexes. \"\"\"\n","        if not self.conditions[condition +'_regex']:\n","            return any(w in self.conditions[condition] for w in words)\n","        else:\n","            return self._match_regex(words, self.conditions[condition+'_regex'])\\\n","                   or any(w in self.conditions[condition] for w in words)\n","\n","    def _is_wordofinterest(self, word, index):\n","        \"\"\" Compare words with the list of words of interest.\n","        Return True if in the list; never accept lacunae or buffers \"\"\"\n","\n","        if self.words[1] == ['*'] and word not in [LINEBREAK, BUFFER]:\n","            return True\n","\n","        if word in [LINEBREAK, BUFFER]:\n","            return False\n","\n","        if not self.regex_words[index]:\n","            return word in self.words[index]\n","        else:\n","            return self._match_regex([word], self.regex_words[index])\\\n","                   or word in self.words[index]\n","\n","    def _is_valid(self, w1, w2, freq):\n","        \"\"\" Validate bigram. Discard stopwords and those which\n","        do not match with the word of interest lists \"\"\"\n","        if freq >= self.minfreq_ab and self.word_freqs[w2] >= self.minfreq_b:\n","            if not self.anywords:\n","                return not self._meets_anycondition('stopwords', [w1, w2])\n","            elif self.anywords and self.anywords2:\n","                return self._is_wordofinterest(w1, 1) and\\\n","                       self._is_wordofinterest(w2, 2)\n","            else:\n","                if self.anywords1:\n","                    return self._is_wordofinterest(w1, 1) and\\\n","                           not self._meets_anycondition('stopwords', [w2])\n","                if self.anywords2:\n","                    return self._is_wordofinterest(w2, 2) and\\\n","                           not self._meets_anycondition('stopwords', [w1])\n","                else:\n","                    return False\n","        else:\n","            return False\n","\n","    def _has_condition(self, window):\n","        \"\"\" Check if conditions are defined. Validate window if true \"\"\"\n","        if not self.anycondition:\n","            return True\n","        else:\n","            if self.positive_condition:\n","                if self._meets_anycondition('conditions', window):\n","                    return True\n","                else:\n","                    return False\n","            elif not self.positive_condition:\n","                if not self._meets_anycondition('conditions', window):\n","                    return True\n","                else:\n","                    return False\n","            else:\n","                print('positive_condition must be True or False')\n","                sys.exit(1)\n","\n","\n","    \"\"\" ================================================================\n","    Bigram counting ====================================================\n","    ================================================================ \"\"\"\n","\n","    def count_bigrams(self):\n","\n","        print(': Counting bigrams')\n","\n","        st = time.time()\n","\n","        \"\"\" Set has_meta if metadata is available. \"\"\"\n","        has_meta = len(self.text.metadata) > 0\n","\n","        text = list(self.text.iterate())\n","\n","        def _check_formulaic(bigram, window, index):\n","            \"\"\" Store windows only if formulaic_measures are used,\n","            otherwise skip this to save memory and time. Remove\n","            index of the collocate from the window to preserve only\n","            context of the bigram \"\"\"\n","            if self.formulaic_measure is not None:\n","                #window.pop(index)\n","                window[index] = LACUNAE[0]\n","                self.WINS.setdefault(bigram, []).append(window)\n","            return bigram\n","\n","        def _gather_meta(bigram, meta):\n","            \"\"\" Get bigram distribution by metadata \"\"\"\n","            self.metadata.setdefault(bigram, {})\n","            self.metadata[bigram].setdefault(meta, 0)\n","            self.metadata[bigram][meta] += 1\n","\n","        def count_bigrams_symmetric():\n","            \"\"\" Symmetric window \"\"\"\n","            wz = self.windowsize - 1\n","            #dupes = []\n","            for w in zip(*[text[i:] for i in range(1+wz*2)]):\n","                #W = [o for o in w if o not in ('_', '<LB>', '<BF>')]\n","                #UNIQ = sorted(list(set(W)))\n","                #if UNIQ == sorted(W):\n","                #    pass\n","                #else:\n","                #    dupes.append(W)\n","\n","                if w[0] == LINEBREAK:\n","                    if has_meta:\n","                        meta = self.text.metadata.pop(0)\n","                if self._is_wordofinterest(w[wz], 1) and \\\n","                   self._has_condition(w[0:wz]+w[wz+1:]):\n","                    for index, bigram in enumerate(itertools.product([w[wz]],\n","                                                    w[0:wz]+w[wz+1:])):\n","                        if has_meta:\n","                            _gather_meta(bigram, meta)\n","                        yield _check_formulaic(bigram,\n","                                               list(w[0:wz]+w[wz+1:]),\n","                                               index)\n","            #for x in dupes:\n","            #    print(x)\n","\n","        def count_bigrams_symmetric_dist():\n","            \"\"\" Symmetric window and distance tracking. \"\"\"\n","\n","            def chain(w1, w2):\n","                \"\"\" Return convolution chain of two lists.\n","                [a, b], [c, d] -> [a, c, b, d] \"\"\"\n","                chain = [' '] * len(w1+w2)\n","                chain[::2] = w1\n","                chain[1::2] = w2\n","                return chain\n","\n","            wz = self.windowsize - 1\n","            for w in zip(*[text[i:] for i in range(1+wz*2)]):\n","                left = list(w[0:wz])\n","                right = list(w[wz+1:])\n","                if w[0] == LINEBREAK:\n","                    if has_meta:\n","                        meta = self.text.metadata.pop(0)\n","                if self._is_wordofinterest(w[wz], 1) and \\\n","                   self._has_condition(left+right):\n","                    for index, bigram in enumerate(itertools.product([w[wz]],\n","                                                    left+right)):\n","                        bigram = _check_formulaic(bigram, left+right, index)\n","                        context = chain(left[::-1], right)\n","                        min_dist = math.floor(context.index(bigram[1])/2) + 1\n","                        if has_meta:\n","                            _gather_meta(bigram, meta)\n","                        self.distances.setdefault(bigram, []).append(min_dist)\n","                        yield bigram\n","\n","        def count_bigrams_forward():\n","            \"\"\" Calculate bigrams within each forward-looking window \"\"\"\n","            for w in zip(*[text[i:] for i in range(self.windowsize)]):\n","                if w[0] == LINEBREAK:\n","                    \"\"\" Keep track of lines and their metadata \"\"\"\n","                    if has_meta:\n","                        meta = self.text.metadata.pop(0)\n","                if self._is_wordofinterest(w[0], 1) \\\n","                        and self._has_condition(w[1:]):\n","                    for index, bigram in enumerate(itertools.product([w[0]],\n","                                                    w[1:])):\n","                        if has_meta:\n","                            \"\"\" If metadata is available, store it \"\"\"\n","                            _gather_meta(bigram, meta)\n","                        yield _check_formulaic(bigram, list(w[1:]), index)\n","\n","        def count_bigrams_forward_dist():\n","            \"\"\" Calculate bigrams within each forward-looking window,\n","            calculate also average distance between words. Distance\n","            tracking is not included into count_bigrams_forward()\n","            for better efficiency \"\"\"\n","            for w in zip(*[text[i:] for i in range(self.windowsize)]):\n","                if w[0] == LINEBREAK:\n","                    if has_meta:\n","                        meta = self.text.metadata.pop(0)\n","                if self._is_wordofinterest(w[0], 1) and self._has_condition(w[1:]):\n","                    for index, bigram in enumerate(itertools.product([w[0]], w[1:])):\n","                        bg = _check_formulaic(bigram, list(w[1:]), index)\n","                        if has_meta:\n","                            _gather_meta(bg, meta)\n","                        d = index + 1\n","                        self.distances.setdefault(bg, []).append(d)\n","                        yield bg\n","\n","        \"\"\" Selector for window type and distance tracking \"\"\"\n","        # TODO: Slice text into smaller parts to prevent memory errors\n","        # when analysing large texts\n","\n","        if self.symmetry:\n","            if self.track_distance:\n","                self.bigram_freqs = Counter(count_bigrams_symmetric_dist())\n","            else:\n","                self.bigram_freqs = Counter(count_bigrams_symmetric())\n","        else:\n","            if self.track_distance:\n","                self.bigram_freqs = Counter(count_bigrams_forward_dist())\n","            else:\n","                self.bigram_freqs = Counter(count_bigrams_forward())\n","\n","        st2 = time.time() - st\n","        IO.show_time(st2, \"Bigram counting\")\n","\n","    def score(self, measure):\n","\n","        \"\"\" Score text by using given measure and return dictionary\n","        containing the results \"\"\"\n","\n","        print(': Scoring bigrams')\n","\n","        st = time.time()\n","\n","        def scale(bf):\n","            \"\"\" Scale bigram frequency with window size to assure\n","            Σ f(a,b) = Σ f(a) = Σ f(b) = N regardless of window size \"\"\"\n","            if WINDOW_SCALING:\n","                if self.symmetry:\n","                    return bf / (self.windowsize - 1) / 2\n","                else:\n","                    return bf / (self.windowsize - 1)\n","            else:\n","                return bf\n","\n","        def apply_weight(ab, a, b, cs, F):\n","            # Apply only to joint-probability\n","            abf = ab * (F**self.factorpower)\n","\n","            return abf, a, b, cs\n","\n","        self.measure = measure.__name__\n","\n","        \"\"\" Declare container for collocation data \"\"\"\n","        scored = {'freqs': {},\n","                  'translations': {},\n","                  'collocations': {},\n","                  'words1': [],\n","                  'words2': []}\n","        w1list, w2list = [], []\n","\n","        \"\"\" Score and store bigrams, translations etc. \"\"\"\n","        F_MEASURE = self.formulaic_measure\n","\n","        # DEBUG: Test that Σ f(a,b) = Σ f(a) = Σ f(b) = N holds\n","        # when calculating I(σ+;σ+)\n","\n","        _abs = 0\n","        _as = sum([v for k, v in self.word_freqs.items() if k not in IGNORE])\n","\n","        \"\"\" Set scoring function according to weighting type \"\"\"\n","        #if self.preweight:\n","        #    SCORE = measure.score_pre_csim\n","        #else:\n","        #    SCORE = measure.score\n","\n","        \"\"\" EXPERIMENTAL:\n","        Estimate number of all bigrams for contingency tables \"\"\"\n","        all_bigrams = scale((self.windowsize-1) * self.corpus_size)\n","\n","        for bigram in self.bigram_freqs.keys():\n","            w1, w2 = bigram\n","\n","            # DEBUG, see above\n","            _abs += scale(self.bigram_freqs[bigram])\n","\n","            if self._is_valid(w1, w2, self.bigram_freqs[bigram]):\n","\n","                freq_w1 = self.word_freqs[w1]\n","                freq_w2 = self.word_freqs[w2]\n","                distance = self._get_distance(bigram)\n","\n","                \"\"\" Apply context similarity measure\"\"\"\n","                if self.formulaic_measure is not None:\n","                    csim_factor = F_MEASURE.score(self.WINS[bigram], w2)\n","                else:\n","                    csim_factor = 0\n","\n","                \"\"\" Smooth for zero-division errors \"\"\"\n","                csim_factor = max(1-csim_factor, 00000.1)\n","\n","                \"\"\" Apply CSW to joint distribution if postweight not selected;\n","                otherwise apply CSW on the final score \"\"\"\n","                if self.formulaic_measure is not None and not self.postweight:\n","                    ab, a, b, cs = apply_weight(self.bigram_freqs[bigram],\n","                                                freq_w1,\n","                                                freq_w2,\n","                                                self.corpus_size,\n","                                                csim_factor)\n","                    factor = 1\n","                else:\n","                    ab, a, b, cs = (self.bigram_freqs[bigram],\n","                                    freq_w1,\n","                                    freq_w2,\n","                                    self.corpus_size)\n","                    factor = csim_factor\n","\n","                score = measure.score(scale(ab), a, b, cs, factor, all_bigrams)\n","\n","                data = {'score': score,\n","                        'distance': distance,\n","                        'frequency': self.bigram_freqs[bigram],\n","                        'similarity': 1-csim_factor,\n","                        'metadata': self.metadata.get(bigram, None)}\n","                scored['translations'][w1] = self._get_translation(w1)\n","                scored['translations'][w2] = self._get_translation(w2)\n","                scored['freqs'][w1] = freq_w1\n","                scored['freqs'][w2] = freq_w2\n","                w1list.append(w1)\n","                w2list.append(w2)\n","                # TODO: use setdefault instead of force\n","                # TODO: rearrange results into printable format on the fly\n","                #       instead of nested JSON which is slow to parse\n","                try:\n","                    scored['collocations'][w1][w2] = data\n","                except KeyError:\n","                    scored['collocations'][w1] = {}\n","                    scored['collocations'][w1][w2] = data\n","                finally:\n","                    pass\n","\n","        # DEBUG, see above: print f(a,b) and f(b) = f(a) if calculated for *\n","        # print('DEBUG:sanity check:',round(_abs), _as)\n","        for k,v in self.bigram_freqs.items():\n","            pass#print(k,v)\n","\n","        \"\"\" Store words of interest for JSON \"\"\"\n","        scored['words1'] = list(set(w1list))\n","        scored['words2'] = list(set(w2list))\n","        scored['minimum'] = measure.minimum\n","\n","        st2 = time.time() - st\n","        IO.show_time(st2, \"Scoring bigrams\")\n","\n","        return scored\n","\n","    \"\"\" ================================================================\n","    Pretty-printing results ============================================\n","    ================================================================ \"\"\"\n","\n","    def _stringify(self, array):\n","        \"\"\" Convert all values in table into strings and\n","        localize decimal markers. \"\"\"\n","        def _format_decimal(item):\n","            if isinstance(item, float):\n","                return str(item).replace('.', DECIMALSEPARATOR)\n","            else:\n","                return str(item)\n","        return [_format_decimal(x) for x in array]\n","\n","    def _sort_by_index(self, table, indices):\n","        \"\"\" Sort table by given two indices, i.e. [0, 4] sorts\n","        the table by 1st and 5th values \"\"\"\n","        i = indices[0]\n","        j = indices[-1]\n","        return sorted(table, key=lambda item: (item[i], item[j]), reverse=True)\n","\n","    def print_matrix(self, scores, value='score', filename=None):\n","        \"\"\" Arguments: ´scores´ dictionary (or JSON) produced by\n","        Associations.score(); ´value´ must be ´score´, ´frequency´ or\n","        ´distance´; set ´filename´ to write output into a file \"\"\"\n","\n","        print(': Building score matrix')\n","        st = time.time()\n","\n","        output = []\n","        heading = [value.upper() + ' W1 -->']\n","        heading += ['{}'.format(w + ' ' + scores['translations'][w])\\\n","                    for w in scores['words2']]\n","        rows = [heading]\n","        for w1 in sorted(scores['words1']):\n","            row = []\n","            for w2 in sorted(scores['words2']):\n","                if HIDE_MIN_SCORE:\n","                    score = ''\n","                else:\n","                    score = scores['minimum']\n","                if w1 in scores['collocations'].keys():\n","                    if w2 in scores['collocations'][w1].keys():\n","                        bigram = scores['collocations'][w1][w2]\n","                        score = bigram.get(value, None)\n","                        if score is None:\n","                            IO.errormsg('bad argument \"%s\" for print_matrix().' % value)\n","                            sys.exit(0)\n","                row.append(self._trim_float(score))\n","            if any(row) and w1 not in LACUNAE + [BUFFER, LINEBREAK]:\n","                rows.append([w1] + row)\n","\n","        \"\"\" Rotate to clean empty columns \"\"\"\n","        for r in [row for row in zip(*rows) if any(row[1:])]:\n","            output.append('\\t'.join([str(x) for x in r]))\n","\n","        st2 = time.time() - st\n","        IO.show_time(st2, \"Building matrix\")\n","\n","        if filename is not None:\n","            IO.write_file(filename, '\\n'.join(output))\n","        else:\n","            print('\\n'.join(output))\n","\n","    def print_scores(self, scores, limit=10000, sortby=('word1', 'score'),\n","                     gephi=False, filename=None):\n","\n","        st = time.time()\n","        print(': Building score table')\n","\n","        def merge_dict(dict1, dict2):\n","           \"\"\" Merge dictionaries and sum the sign frequencies \"\"\"\n","           combined = {**dict1, **dict2}\n","           for key, value in combined.items():\n","               if key in dict1 and key in dict2:\n","                   combined[key] = value + dict1[key]\n","           return combined\n","\n","        def collate_meta(meta):\n","            \"\"\" Combine multi-value metadata; e.g. SB|Nippur|Literary\n","            will be split into three additional subdictionaries and\n","            frequency data is collated. \"\"\"\n","\n","            # TODO: Aliases\n","\n","            if meta is None:\n","                return meta\n","\n","            vals = len(list(meta.keys())[0].split('|'))\n","            original = {i+1:{} for i in range(0, vals)}\n","            original[0] = meta\n","            for key, val in meta.items():\n","                for slot, k in enumerate(key.split('|')):\n","                    original[slot+1] = merge_dict({k: val}, original[slot+1])\n","            return original\n","\n","        header = ['word1',  'word2',  'word1 freq', 'word2 freq',\n","                  'bigram freq', 'score', 'distance', 'similarity', 'url']\n","        sort_indices = [header.index(x) for x in sortby]\n","\n","        \"\"\" Set headers \"\"\"\n","        if gephi:\n","            output = [(';'.join(['source', 'target', 'weight']))]\n","        else:\n","            output = ['\\t'.join([x for x in header])]\n","\n","        rows = []\n","        for w1 in scores['collocations'].keys():\n","            for w2 in scores['collocations'][w1].keys():\n","                bigram = scores['collocations'][w1][w2]\n","                freqs = scores['freqs']\n","                meta = collate_meta(bigram['metadata'])\n","                items = {'word1': w1,\n","                         'word2': w2,\n","                         #'attr1': scores['translations'][w1],\n","                         #'attr2': scores['translations'][w2],\n","                         'word1 freq': freqs[w1],\n","                         'word2 freq': freqs[w2],\n","                         'bigram freq': bigram['frequency'],\n","                         #'metaa': meta,\n","                         'score':\n","                             float(self._trim_float(bigram['score'])),\n","                         'distance': bigram['distance'],\n","                         'similarity': self._trim_float(bigram['similarity']),\n","                         'url': make_korp_oracc_url(w1, w2, self.windowsize-2)}\n","                rows.append([items[key] for key in header])\n","\n","        \"\"\" Sort and convert into tsv \"\"\"\n","        lastword = ''\n","        for line in self._sort_by_index(rows, sort_indices):\n","            if not gephi:\n","                if lastword != line[0]:\n","                    i = 0\n","                if i < limit:\n","                    output.append('\\t'.join(self._stringify(line)))\n","                lastword = line[0]\n","                i += 1\n","            if gephi:\n","                if lastword != line[0]:\n","                    i = 0\n","                if i < limit:\n","                    data = [line[0]+' '+line[1]] + [line[2]+' '+line[3]+' ('+str(line[5]) +')'] + [line[7]]\n","                    output.append('\\t'.join(self._stringify(data)))\n","                lastword = line[0]\n","                i += 1\n","\n","\n","        st2 = time.time() - st\n","        IO.show_time(st2, \"Building score table\")\n","\n","        if filename is None:\n","            print('\\n'.join(output))\n","        else:\n","            IO.write_file(filename, '\\n'.join(output))\n","\n"]},{"cell_type":"markdown","id":"845fb537","metadata":{"id":"845fb537"},"source":["## Set options and run the script\n","\n","**This is the part of the script that can be safely modified.**\n","\n","**DEFINING KEYWORDS:**\n","\n","* ```python\n","words1=['sisû[horse]N', 'parû[mule]N']\n","```\n","Would find all collocates for *sisû* and *parû*. You can also use regular expressions but they must be precompiled, for example.\n","\n","* ```python\n","words1=[re.compile('banû.+')]\n","```\n","Will find collocates for *banû* regarldess of their sense or part-of-speech. If you want to only find *banûs* that are adjectives, you can do\n","\n","* ```python\n","words1=[re.compile('banû.+AJ')]\n","```\n","\n","\n","**DEFINING CLOSED COLLOCATE SETS**\n","\n","You can limit your possible collocates by defining words2. For example, if you only want to find collocates that are divine names, use\n","\n","* ```python\n","words2=[re.compile('.*DN.*')]\n","```\n","\n","\n","**DEFINING STOPWORDS**\n","\n","Stopwords can be used to filter out groups of words from your collocate lists. For example, if you want to filter out all proper names and royal names, you can use the regular expression [RD]N, that is (RN|DN).\n","\n","* ```python\n","stopwords=[re.compile('.*([RD]N).*')]\n","```\n","\n","In general its faster to define stopwords in inverse by using words2, e.g.\n","\n","* ```python\n","words2=[re.compile('.*\\](N|AJ|V).*')]\n","```\n","\n","disallows *ALL* collocates other than nouns, adjectives and verbs.\n","\n","\n","**DEFINING CONDITIONS (ADVANCED FEATURE)**\n","\n","To define conditions for your collocates, you can use arguments `conditions` and `positive_condition`. For example, the following arguments\n","\n","* ```python\n","words1=['kakku[weapon]N'],\n","conditions=[re.compile('.*naparšudu.*')],\n","positive_condition=False,\n","```\n","will calculate collocates for *kakku[weapon]N ONLY* if the word *naparšudu* does not exist within the window.\n","\n","On the contrary, the following would find collocates for *kakku[weapon]N* only if *naparšudu* exists in the window.\n","\n","* ```python\n","words1=['kakku[weapon]N'],\n","    conditions=[re.compile('.*naparšudu.*')],\n","    positive_condition=True,\n","```\n","\n","\n","**AUTO-GENERATING STOPWORD LISTS (ADVANCED FEATURE)**\n","   \n","* ```python\n","stopwords=z.tf_idf(threshold=1000)\n","```\n","Will generate a list of 1000 most uninteresting words in the text by using TF-IDF. Note that this is a part of the Text class."]},{"cell_type":"code","source":["\n","def fetch_file_from_github(url):\n","    # Fetch the content from URL\n","    response = requests.get(url)\n","    response.raise_for_status()  # Ensure we catch HTTP errors\n","\n","    # Create a temporary file to write the content\n","    temp_file = tempfile.NamedTemporaryFile(delete=False, mode='w+', encoding='utf-8')\n","    temp_file.write(response.text)\n","    temp_file.close()  # Close the file to ensure it can be reopened by IO.read_file\n","\n","    return temp_file.name  # Return the path of the temporary file\n"],"metadata":{"id":"D1KQeEvorn4m","executionInfo":{"status":"ok","timestamp":1716371208936,"user_tz":-180,"elapsed":300,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"id":"D1KQeEvorn4m","execution_count":20,"outputs":[]},{"cell_type":"code","source":["# URL of the raw file on GitHub\n","url = 'https://raw.githubusercontent.com/asahala/Pmizer/master/data/dataset.txt'\n","\n","temp_filename = fetch_file_from_github(url)"],"metadata":{"id":"jD1m1bKLrrCO","executionInfo":{"status":"ok","timestamp":1716371213411,"user_tz":-180,"elapsed":951,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}}},"id":"jD1m1bKLrrCO","execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":25,"id":"505c02b1","metadata":{"ExecuteTime":{"end_time":"2024-05-21T20:23:14.256824Z","start_time":"2024-05-21T20:23:14.164228Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"505c02b1","executionInfo":{"status":"ok","timestamp":1716371857572,"user_tz":-180,"elapsed":323,"user":{"displayName":"shai gordin","userId":"12645290687300756012"}},"outputId":"48b983b6-1168-4954-c867-ef1cd9bb7cd6"},"outputs":[{"output_type":"stream","name":"stdout","text":[": Reading /tmp/tmpcj9t1t99\n","    Reading took 0.05 seconds\n","\n",": Text statistics:\n","    Lines: 432\n","    Longest line: 6147\n","    Word count: 88252\n","    Word count (non-lacunae): 62469\n","    Lacunae or ignored symbols: 25783\n","    Unique words: 4149\n","    Median word frequency: 4\n","    Average word frequency: 21.47\n","\n",": Counting bigrams\n","    Bigram counting took 0.09 seconds\n",": Scoring bigrams\n","    Scoring bigrams took 0.00 seconds\n",": Building score table\n","    Building score table took 0.00 seconds\n",": Saved PMI_results.tsv\n"]}],"source":["#z = Text('data/PMI_dataset.txt') # Dataset in TPL (text per line) format\n","z = Text(temp_filename)\n","\n","wz = 5 # Window size\n","x = Associations(z,\n","                 words1=['maddattu[payment]N'],      # keyword of interest\n","                 #conditions=[re.compile('.*naparšudu.*')],\n","                 #positive_condition=False,\n","                 formulaic_measure=Lazy,   # use CSW\n","                 minfreq_b = 2,            # min collocate freq\n","                 minfreq_ab = 2,           # min bigram freq\n","                 symmetry=True,            # use symmetric window\n","                 windowsize=wz,            # window size\n","                 factorpower=3)            # CSW k-value\n","\n","A = x.score(PMIDELTA)              # Select measure (e.g, PMI, PMI2, Jaccard...)\n","\n","# Save results\n","x.print_scores(A, limit=20, gephi=False, filename='PMI_results.tsv')"]},{"cell_type":"markdown","id":"614ee526","metadata":{"ExecuteTime":{"end_time":"2024-05-21T19:12:42.974770Z","start_time":"2024-05-21T19:12:42.963847Z"},"id":"614ee526"},"source":["## Critical bug fixes\n","\n","\n","        2019-06-26: changed buffering from the end of the line\n","                    to the beginning to prevent rare crash that\n","                    occurred if the keyword was coincidentally\n","                    in the middle of the first symmetric window.\n","                    now linebreak is always the first symbol in text.\n","\n","        2020-02-10: lazy context similarity algorithm now preserves\n","                    lacuna positions. other algorithms are not\n","                    recommended as they are not fixed yet.\n","\n","        2020-03-03: regular expressions now work properly in forward-\n","                    looking windows.\n","\n","                    symmetric window scaling is now correct and\n","                    Σ f(a,b) = Σ f(a) = Σ f(b) = N when I(σ+;σ+)\n","                    where σ is symbol of the alphabet.\n","\n","        2020-05-20: Fix incorrect bounds for PMI3. Lazy context\n","                    similarity measure now discards collocates\n","                    from counts properly (i.e. subtracts 1 from\n","                    the denominator).\n","\n","        2024-05-19: Fix links to Korp.\n","        \n","\n","## Other fixes\n","\n","        2020-11-27: Preweight is now default.\n","\n","        2020-01-01: Additional measures such as Jaccard etc."]},{"cell_type":"code","execution_count":null,"id":"70f1bc61","metadata":{"id":"70f1bc61"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}